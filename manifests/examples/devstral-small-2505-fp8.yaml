apiVersion: vllm.sir-alfred.io/v1alpha1
kind: VLLMModel
metadata:
  name: devstral-small-2505-fp8
spec:
  # Model Identification
  modelName: "nm-testing/Devstral-Small-2505-FP8-dynamic"
  servedModelName: "devstral-small-2505-fp8"

  # Parsing Configuration
  toolCallParser: "mistral"
  reasoningParser: ""
  tokenizerMode: "mistral"
  chatTemplate: "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 and system_message %}{{ '[INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}"

  # vLLM Runtime Parameters (model-specific)
  # Note: GPU count and CPU offload are configured at vllm-chill deployment level (GPU_COUNT and CPU_OFFLOAD_GB env vars)
  # Devstral-Small-2505: 24B params, FP8-dynamic quantization, 128k context window
  # Optimized for agentic coding tasks
  # Reduced maxModelLen to 32768 to fit in available GPU memory
  maxModelLen: 32768
  gpuMemoryUtilization: 0.90
  enableChunkedPrefill: true
  maxNumBatchedTokens: 8192
  maxNumSeqs: 16
  dtype: "auto"
  disableCustomAllReduce: true
  enablePrefixCaching: true
  enableAutoToolChoice: true
