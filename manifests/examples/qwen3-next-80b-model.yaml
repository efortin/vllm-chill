apiVersion: vllm.sir-alfred.io/v1alpha1
kind: VLLMModel
metadata:
  name: qwen3-next-80b-fp8
spec:
  # Model Identification
  modelName: "Qwen/Qwen3-Next-80B-A3B-Instruct-FP8"
  servedModelName: "qwen3-next-80b-fp8"

  # Parsing Configuration
  toolCallParser: "qwen3_coder"
  reasoningParser: ""

  # GPU Configuration
  gpuCount: 2

  # vLLM Runtime Parameters
  maxModelLen: 110000
  gpuMemoryUtilization: 0.91
  enableChunkedPrefill: true
  maxNumBatchedTokens: 8192
  maxNumSeqs: 16
  dtype: "float16"
  disableCustomAllReduce: true
  enablePrefixCaching: true
  cpuOffloadGB: 0
  enableAutoToolChoice: true
