apiVersion: vllm.sir-alfred.io/v1alpha1
kind: VLLMModel
metadata:
  name: glm-4.5-fp8
spec:
  # Model Identification
  modelName: "zai-org/GLM-4.5-FP8"
  servedModelName: "glm-4.5-fp8"

  # Parsing Configuration
  toolCallParser: "glm45"
  reasoningParser: "glm45"
  quantization: "bitsandbytes-int8"

  # vLLM Runtime Parameters (model-specific)
  # Note: GPU count and CPU offload are configured at vllm-chill deployment level (GPU_COUNT and CPU_OFFLOAD_GB env vars)
  # GLM-4.5-FP8: 355B params with A32B active, FP8 quantization, 128K context window
  # Hybrid reasoning model with thinking mode for complex reasoning and tool usage
  # WARNING: This is a very large model. For 2x 24GB GPUs, context length is reduced significantly
  # Recommended: 8x H100 or 4x H200 for full 128K context
  maxModelLen: 32768
  gpuMemoryUtilization: 0.85
  enableChunkedPrefill: false
  maxNumBatchedTokens: 2048
  maxNumSeqs: 2
  dtype: "float16"
  disableCustomAllReduce: true
  enablePrefixCaching: true
  enableAutoToolChoice: true
